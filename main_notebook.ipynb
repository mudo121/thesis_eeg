{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Testing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import os, sys\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Local Imports\n",
    "module_path = os.path.abspath(os.path.join('code'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from Signal_Transfomers import (ConvertIndexToTimestamp, ExtractSignals,\n",
    "                                BandpassFilter, BandstopFilter, ReplaceOutliers,\n",
    "                                CenterData)\n",
    "\n",
    "from Pre_Processing_Transformers import (SlidingWindow, NormalizeData, DeleteFaultyEpochs,\n",
    "                                            ReplaceNaNs)\n",
    "\n",
    "\n",
    "from Feature_Extraction_Transformer import (Frequency_Features)\n",
    "\n",
    "from Measuring_Functions import (getChannelUsageInEpochSeries)\n",
    "\n",
    "from plotFunctions import (plotInteractiveEpochs, plotFeatureEpochs)\n",
    "from consts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define local functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadConfigFile(configFilePath : str) -> Dict:\n",
    "    with open(configFilePath, 'r') as stream:\n",
    "        try:\n",
    "            yamlConfig = yaml.safe_load(stream)\n",
    "            return yamlConfig\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "            return None\n",
    "\n",
    "def readFileCSV(filePath : str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(filePath)\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_signal(df : pd.DataFrame, config : Dict, starttime=None) -> pd.DataFrame:\n",
    "    ''' Filter the signal with bandpass, bandstopp and repace outliers '''\n",
    "\n",
    "    # signal processing pipeline - the first pipeline - e.g. extract the signal from the raw .csv and filter it\n",
    "    signal_processing_pipeline = Pipeline([\n",
    "        ('Convert Index to Timestamp', ConvertIndexToTimestamp(device=config['deviceName'], starttime=starttime)),\n",
    "        ('Extract signals', ExtractSignals(device=config['deviceName'])),\n",
    "        ('Bandpass Filter', BandpassFilter(device=config['deviceName'], lowcufreq=config['lowcutFreq_bandpass'], highcutfreq=config['highcutFreq_bandpass'], samplingRate=config['samplingRate'])),\n",
    "        ('Bandstop Filter', BandstopFilter(device=config['deviceName'], lowcufreq=config['lowcutFreq_bandstopp'], highcutfreq=config['highcutFreq_bandstopp'], samplingRate=config['samplingRate'])),\n",
    "        ('Replace Outliers', ReplaceOutliers(device=config['deviceName'], lowerThreshold=config['lowerThreshold'], upperThreshold=config['upperThreshold']))\n",
    "    ])\n",
    "    df = signal_processing_pipeline.fit_transform(df)\n",
    "    return df\n",
    "\n",
    "def pre_process_signal(df : pd.DataFrame, config : Dict) -> pd.Series:\n",
    "    ''' Pre-process the signal by creating epochs, delete faulty epochs and normalize it\n",
    "    \n",
    "    Returns a series of dataframes\n",
    "    '''\n",
    "    # pre-process the pipeline for machine learning\n",
    "    pre_processing_pipeline = Pipeline([\n",
    "        ('Create Epochs', SlidingWindow(samplingRate=config['samplingRate'], windowSizeInSeconds=config['epochWindowSize'], overlapInSeconds=config['overlap'])),\n",
    "        ('Delete Faulty Epochs', DeleteFaultyEpochs(maxFaultyRate=config['maxFaultyRate'])), # returns a numpy series with dataframes\n",
    "        ('Replace NaNs with Zero', ReplaceNaNs()),\n",
    "        ('Normalize Data', NormalizeData())\n",
    "    ])\n",
    "\n",
    "    epochSeries = pre_processing_pipeline.fit_transform(df)\n",
    "    return epochSeries\n",
    "\n",
    "def feature_extraction(epochSeries : pd.Series, config : Dict) -> pd.Series:\n",
    "    ''' Extract features from the generated epoch series \n",
    "    \n",
    "    Do the whole feature extraction in a pipeline\n",
    "    '''\n",
    "\n",
    "    feature_extraction_pipeline = Pipeline([\n",
    "        (\"Frequency Band feature extraction\", Frequency_Features(samplingRate=config['samplingRate'], frequencyBands=config['frequencyBands'],\n",
    "                                                                numberOfChannels=config['numberOfChannels'], epochSizeCalculation=config['epochSizeCalculation']))\n",
    "    ])\n",
    "\n",
    "    epochSeries = feature_extraction_pipeline.fit_transform(epochSeries)\n",
    "\n",
    "    return epochSeries\n",
    "\n",
    "def generateFeatureDf(csvFilePath, starttime, yamlConfig, label : str ) -> pd.DataFrame:\n",
    "    ''' Generates datframes of given csv filepaths'''\n",
    "    \n",
    "    print (\"Generating {} driving feature df...\".format(label))\n",
    "    \n",
    "    # load dataset\n",
    "    df = readFileCSV(csvFilePath)  \n",
    "    \n",
    "    # Filter the signal\n",
    "    df = filter_signal(df=df, config=yamlConfig, starttime=starttime)\n",
    "\n",
    "    # Pre-process the signal\n",
    "    epochSeries = pre_process_signal(df=df, config=yamlConfig)\n",
    "    # Save epochSeries\n",
    "    print(\"Saving epoch series...\")\n",
    "    epochSeries.to_pickle(os.path.join(mainDir,'generatedData','epochSeries_{}.pkl'.format(label)))\n",
    "\n",
    "\n",
    "    # Extract Frequency Features\n",
    "    frequencyFeatureDf = feature_extraction(epochSeries=epochSeries, config=yamlConfig)\n",
    "    # Save frequency features dataframe\n",
    "    print(\"Saving frequency feature dataframe...\")\n",
    "    frequencyFeatureDf.to_pickle(os.path.join(mainDir,'generatedData','frequencyFeaturesDf_{}.pkl'.format(label)))\n",
    "    \n",
    "    return frequencyFeatureDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Frequency Feature Df has 182 Epochs and 570 Features.\n"
     ]
    }
   ],
   "source": [
    "GENERATE_DATA = False\n",
    "# load config\n",
    "mainDir = \"D:/Masterthesis/thesis_eeg/\"\n",
    "\n",
    "TARGET_AWAKE = \"awake\"\n",
    "TARGET_NON_AWAKE = \"non_awake\"\n",
    "TARGET_UNLABELED = \"unlabeled\"\n",
    "\n",
    "configFilePath = \"D:/Masterthesis/thesis_eeg/config/openBci.yaml\"\n",
    "yamlConfig = loadConfigFile(configFilePath)\n",
    "#print(yamlConfig)\n",
    "\n",
    "subjectDict = {\"subjectId\" : 1,\n",
    "               \"awakeCsvPath\" : \"D:/OneDrive - bwedu/Masterthesis/Experiments+Data/Fahren+AimLab/2020_03_05_Propand_1/openBci_record-[2020.03.05-12.27.35]_raw_awake_aimlab.csv\",\n",
    "               \"unlabeledCsvPath\" : \"D:/OneDrive - bwedu/Masterthesis/Experiments+Data/Fahren+AimLab/2020_03_05_Propand_1/openBci_record-[2020.03.05-12.34.34]_raw_driving_unlabled.csv\"}\n",
    "\n",
    "if GENERATE_DATA:\n",
    "    print(\"Generating data...\")\n",
    "    \n",
    "    # create awake feature df\n",
    "    frequencyFeatureDf_awake = generateFeatureDf(csvFilePath= subjectDict['awakeCsvPath'],\n",
    "                                                 starttime = pd.Timestamp(datetime.strptime('[2020.03.05-12.27.27]', \"[%Y.%m.%d-%H.%M.%S]\")),\n",
    "                                                 yamlConfig = yamlConfig,\n",
    "                                                 label = TARGET_AWAKE)\n",
    "\n",
    "    # create undetermined feature df\n",
    "    frequencyFeatureDf_unlabeled = generateFeatureDf(csvFilePath= subjectDict['unlabeledCsvPath'],\n",
    "                                                 starttime = pd.Timestamp(datetime.strptime('[2020.03.05-12.34.34]', \"[%Y.%m.%d-%H.%M.%S]\")),\n",
    "                                                 yamlConfig = yamlConfig,\n",
    "                                                 label = TARGET_UNLABELED)\n",
    "    \n",
    "    \n",
    "\n",
    "else:\n",
    "    # load data\n",
    "    print(\"Loading data...\")\n",
    "    # ---- AWAKE DATA -----\n",
    "    #epochSeries_awake = pd.read_pickle(os.path.join(mainDir,'generatedData','epochSeries_{}.pkl'.format(TARGET_AWAKE)))\n",
    "    frequencyFeatureDf_awake = pd.read_pickle(os.path.join(mainDir,'generatedData','frequencyFeaturesDf_{}.pkl'.format(TARGET_AWAKE)))\n",
    "    \n",
    "    # ---- UNLABELED DATA -----\n",
    "    #epochSeries_unlabeled = pd.read_pickle(os.path.join(mainDir,'generatedData','epochSeries_{}.pkl'.format(TARGET_UNLABELED)))\n",
    "    frequencyFeatureDf_unlabeled = pd.read_pickle(os.path.join(mainDir,'generatedData','frequencyFeaturesDf_{}.pkl'.format(TARGET_UNLABELED)))\n",
    "    \n",
    "    \n",
    "    # ---- Create NON-WAKE DATA ----\n",
    "    frequencyFeatureDf_non_awake = frequencyFeatureDf_unlabeled[110:160] # 50 epochs = 50 x 5 seconds = 250 seconds\n",
    "    frequencyFeatureDf_non_awake.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#print(epochSeries[0].head())\n",
    "#plt.show(epochSeries[0].plot())\n",
    "\n",
    "\n",
    "\n",
    "print(\"Frequency Feature Df has {} Epochs and {} Features.\".format(frequencyFeatureDf_unlabeled.shape[0],\n",
    "                                                                    frequencyFeatureDf_unlabeled.shape[1],\n",
    "                                                                    yamlConfig['epochSizeCalculation']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = {\"awake\" : 1, \"non-awake\" : 0, \"unlabeled\" : 2}\n",
    "\n",
    "def createDataAndTargetArray(awakeDf, non_awakeDf, unlabeledDf):\n",
    "    targetArray = []\n",
    "    dataDf = pd.DataFrame()\n",
    "    \n",
    "    if awakeDf is not None:\n",
    "        \n",
    "        # TODO - Checken ob gut oder schlecht?!?!?\n",
    "        awakeDf = awakeDf.fillna(0)\n",
    "        \n",
    "        startCounter = 0\n",
    "        if dataDf.empty:\n",
    "            # Do this because then it will copy also all columns, then we can append stuff\n",
    "            dataDf = awakeDf.loc[:1]\n",
    "            \n",
    "            # append to awake into the target array\n",
    "            targetArray.append(target_dict['awake'])\n",
    "            targetArray.append(target_dict['awake'])\n",
    "            startCounter = 2\n",
    "            \n",
    "        # We have to start at 2 if we added 0:1 already\n",
    "        for i in range(startCounter, len(awakeDf)):\n",
    "            dataDf = dataDf.append(awakeDf.loc[i], ignore_index=True)\n",
    "            targetArray.append(target_dict['awake'])\n",
    "\n",
    "            \n",
    "    if non_awakeDf is not None:\n",
    "        \n",
    "        # TODO - Checken ob gut oder schlecht?!?!?\n",
    "        non_awakeDf = non_awakeDf.fillna(0)\n",
    "        \n",
    "        startCounter = 0\n",
    "        if dataDf.empty:\n",
    "            # Do this because then it will copy also all columns, then we can append stuff\n",
    "            dataDf = non_awakeDf.loc[:1]\n",
    "            \n",
    "            # append to awake into the target array\n",
    "            targetArray.append(target_dict['non-awake'])\n",
    "            targetArray.append(target_dict['non-awake'])\n",
    "            startCounter = 2\n",
    "        \n",
    "        # We have to start at 2 if we added 0:1 already\n",
    "        for i in range(startCounter, len(non_awakeDf)):\n",
    "            dataDf = dataDf.append(non_awakeDf.loc[i], ignore_index=True)\n",
    "            targetArray.append(target_dict['non-awake'])\n",
    "            \n",
    "            \n",
    "    if unlabeledDf is not None:\n",
    "        \n",
    "        # TODO - Checken ob gut oder schlecht?!?!?\n",
    "        unlabeledDf = unlabeledDf.fillna(0)\n",
    "        \n",
    "        startCounter = 0\n",
    "        if dataDf.empty:\n",
    "            # Do this because then it will copy also all columns, then we can append stuff\n",
    "            dataDf = unlabeledDf.loc[:1]\n",
    "            \n",
    "            # append to awake into the target array\n",
    "            targetArray.append(target_dict['unlabeled'])\n",
    "            targetArray.append(target_dict['unlabeled'])\n",
    "            startCounter = 2\n",
    "        \n",
    "        # We have to start at 2 if we added 0:1 already\n",
    "        for i in range(startCounter, len(unlabeledDf)):\n",
    "            dataDf = dataDf.append(unlabeledDf.loc[i], ignore_index=True)\n",
    "            targetArray.append(target_dict['unlabeled'])\n",
    "\n",
    "            \n",
    "    return (dataDf.to_numpy(), np.array(targetArray))\n",
    "\n",
    "dataArray, targetArray = createDataAndTargetArray(awakeDf = frequencyFeatureDf_awake,\n",
    "                                               non_awakeDf = frequencyFeatureDf_non_awake,\n",
    "                                               unlabeledDf = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataArray, targetArray, test_size=0.3,random_state=109, shuffle=False) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "#Create a svm Classifier\n",
    "\n",
    "clf = svm.SVC(kernel=\"poly\", degree=3, coef0=1, C=5, gamma=\"auto\")\n",
    "\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = frequencyFeatureDf_unlabeled.fillna(0)\n",
    "test = test.to_numpy()\n",
    "clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reiss\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82758621, 0.85714286, 0.85185185])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(clf, X_train, y_train, cv=3)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 13],\n",
       "       [ 0, 71]], dtype=int64)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109) # 70% training and 30% test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
