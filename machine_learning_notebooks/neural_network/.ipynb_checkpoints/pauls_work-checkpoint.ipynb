{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Masterthesis\\thesis_eeg\\code\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# to enable local imports\n",
    "module_path = os.path.abspath('../../code')\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from machine_learning_load_data import loadOnlineEEGdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Online EEG Data from ../../../EEG_Data/eeg_data_online ...\n",
      "EEG Data Shape:\n",
      "(5024, 512, 40) (5024,) (2154, 512, 40) (2154,)\n",
      "Freq Data Shape:\n",
      "(1008, 1, 1200) (1008,) (432, 1, 1200) (432,)\n",
      "Entropy Data Shape:\n",
      "(5024, 1, 200) (5024,) (2154, 1, 200) (2154,)\n",
      "Loading Online EEG Data from ../../../EEG_Data/muse_data ...\n",
      "EEG Data Shape:\n",
      "(5393, 512, 4) (5393,) (2312, 512, 4) (2312,)\n",
      "Freq Data Shape:\n",
      "(1084, 1, 120) (1084,) (465, 1, 120) (465,)\n",
      "Entropy Data Shape:\n",
      "(5393, 1, 20) (5393,) (2312, 1, 20) (2312,)\n"
     ]
    }
   ],
   "source": [
    "# Load some online EEG Data\n",
    "eegData, freqData, entropyData = loadOnlineEEGdata(dirPath='../../../EEG_Data/eeg_data_online', splitData=True)\n",
    "\n",
    "X_train_eeg, y_train_eeg, X_test_eeg, y_test_eeg = eegData\n",
    "X_train_freq, y_train_freq, X_test_freq, y_test_freq = freqData\n",
    "X_train_entropy, y_train_entropy, X_test_entropy, y_test_entropy = entropyData\n",
    "\n",
    "# reshape\n",
    "X_train_freq = X_train_freq.reshape(X_train_freq.shape[0], X_train_freq.shape[2])\n",
    "X_test_freq = X_test_freq.reshape(X_test_freq.shape[0], X_test_freq.shape[2])\n",
    "\n",
    "X_train_entropy = X_train_entropy.reshape(X_train_entropy.shape[0], X_train_entropy.shape[2])\n",
    "X_test_entropy = X_test_entropy.reshape(X_test_entropy.shape[0], X_test_entropy.shape[2])\n",
    "\n",
    "\n",
    "##################\n",
    "# experiment data\n",
    "##################\n",
    "eegData_exp, freqData_exp, entropyData_exp = loadOnlineEEGdata(dirPath='../../../EEG_Data/muse_data', splitData=True)\n",
    "\n",
    "X_train_eeg_exp, y_train_eeg_exp, X_test_eeg_exp, y_test_eeg_exp = eegData_exp\n",
    "X_train_freq_exp, y_train_freq_exp, X_test_freq_exp, y_test_freq_exp = freqData_exp\n",
    "X_train_entropy_exp, y_train_entropy_exp, X_test_entropy_exp, y_test_entropy_exp = entropyData_exp\n",
    "\n",
    "# reshape\n",
    "X_train_freq_exp = X_train_freq_exp.reshape(X_train_freq_exp.shape[0], X_train_freq_exp.shape[2])\n",
    "X_test_freq_exp = X_test_freq_exp.reshape(X_test_freq_exp.shape[0], X_test_freq_exp.shape[2])\n",
    "\n",
    "X_train_entropy_exp = X_train_entropy_exp.reshape(X_train_entropy_exp.shape[0], X_train_entropy_exp.shape[2])\n",
    "X_test_entropy_exp = X_test_entropy_exp.reshape(X_test_entropy_exp.shape[0], X_test_entropy_exp.shape[2])\n",
    "\n",
    "#targetLabelsDict_exp = loadTargetLabelsTxt(filePath='../../EEG_Data/muse_data/target_labels.txt')\n",
    "#targetNames_exp = ['AWAKE ({})'.format(targetLabelsDict_exp['AWAKE']),\n",
    "#               'FATIGUE ({})'.format(targetLabelsDict_exp['FATIGUE'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainEvaluateNet(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    #Convert data set to numpy array of type float32\n",
    "    #x_train = np.asarray(x_train, np.float32)\n",
    "    #y_train = np.asarray(y_train, np.float32)\n",
    "    #x_test = np.asarray(x_test, np.float32)\n",
    "    #y_test = np.asarray(y_test, np.float32)\n",
    "\n",
    "    #Number of neurons\n",
    "    neurons_num = int((2/3) * x_train.shape[2])\n",
    "\n",
    "    #Initialize neural net\n",
    "    #output of layer = keras.layers.Dense(number of neurons, activation function, name)(input of layer)\n",
    "    inputs = keras.Input(shape=(x_train.shape[1], x_train.shape[2]), name='input_points')\n",
    "    x = keras.layers.Dense(neurons_num, activation='relu', name='L1')(inputs)\n",
    "    x = keras.layers.Dense(neurons_num, activation='relu', name='L2')(x)\n",
    "    outputs = keras.layers.Dense(2, activation='softmax', name='output_point')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='fatigue_classifier')\n",
    "\n",
    "    #Set optimizer, loss function and metrics to track\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    #binary_crossentropy\n",
    "\n",
    "    #Summary of the neural net\n",
    "    #model.summary()\n",
    "    \n",
    "    #Train neural net\n",
    "    model.fit(x_train, y_train, epochs=3)\n",
    "\n",
    "    #Evaluate neural net\n",
    "    score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "    loss = score[0]\n",
    "    accuracy = score[1]\n",
    "    print('Test loss:', loss)\n",
    "    print('Test accuracy:', accuracy)\n",
    "    return (model, loss, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEvaluateNet(x_train, y_train, x_test, y_test,batch_size=64, num_classes=2, epochs=5, verbose=0):\n",
    "\n",
    "    #Settings\n",
    "    neurons_num = int((2/3) * x_train.shape[2]) #Number of neurons\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    #Initialize neural net\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=10, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(Conv1D(64, kernel_size=1, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neurons_num, activation='relu'))\n",
    "    model.add(Dense(neurons_num, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax', name='last_dense')) \n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    #Set optimizer, loss function and metrics to track\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(0.01), metrics=['accuracy'])\n",
    "    #model.compile(loss=keras.losses.squared_hinge, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "    #Summary of the neural net\n",
    "    #model.summary()\n",
    "    \n",
    "    #Train neural net\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=verbose,\n",
    "              validation_data=(x_test, y_test))\n",
    "    #         validation_split=0.2)\n",
    "\n",
    "    #Evaluate neural net\n",
    "    score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "    loss = score[0]\n",
    "    accuracy = score[1]\n",
    "    print('Test loss: {}'.format(loss))\n",
    "    print('Test accuracy: {}'.format(accuracy))\n",
    "    return (model, loss, accuracy)\n",
    "\n",
    "    #Predict best route\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEasyNet(x_train, y_train, x_test, y_test, batch_size=64, num_classes=2, epochs=5, verbose=0):\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    neurons_num = int((2/3) * x_train.shape[1]) #Number of neurons\n",
    "    \n",
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(neurons_num*2), input_dim=x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(neurons_num, activation='relu'))\n",
    "    #model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(int(neurons_num/2), activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax', name='last_dense'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    # compile the keras model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(0.01), metrics=['accuracy'])\n",
    "    # fit the keras model on the dataset\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    #Evaluate neural net\n",
    "    score = model.evaluate(x_test, y_test, verbose=verbose, batch_size=batch_size)\n",
    "    loss = score[0]\n",
    "    accuracy = score[1]\n",
    "    #print('Test loss: {}'.format(loss))\n",
    "    #print('Test accuracy: {}'.format(accuracy))\n",
    "    return (model, loss, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def train_lstm(x_train, y_train, x_test, y_test, batch_size=64, num_classes=2, epochs=5, verbose=0):\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    try:\n",
    "        n_timesteps, n_features, n_outputs = x_train.shape[1], x_train.shape[2], y_train.shape[1]\n",
    "    except IndexError: \n",
    "        # reshape that...\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "        n_timesteps, n_features, n_outputs = x_train.shape[1], x_train.shape[2], y_train.shape[1]\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(\"n_timesteps: {} - n_features: {} - n_outputs: {}\".format(n_timesteps, n_features, n_outputs))\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    #Evaluate neural net\n",
    "    score = model.evaluate(x_test, y_test, verbose=verbose, batch_size=batch_size)\n",
    "    loss = score[0]\n",
    "    accuracy = score[1]\n",
    "    #print('Test loss: {}'.format(loss))\n",
    "    #print('Test accuracy: {}'.format(accuracy))\n",
    "    return (model, loss, accuracy)\n",
    "\n",
    "def train_cnn(x_train, y_train, x_test, y_test, batch_size=64, num_classes=2, epochs=5, verbose=0):\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "    \n",
    "    # Create the network\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(40, 10, strides=2, padding='same', activation='relu', input_shape=(x_train.shape[0], x_train.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(40, 5, strides=2, padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(40, 4, strides=1, padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train and save results for later plotting\n",
    "    #history[ '40 1023 523 413 50 :'+'-'.join(selected)] = model.fit(x_train, y_train, \n",
    "    #    batch_size=100, epochs=40, validation_data=(x_val,y_val))\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    #Evaluate neural net\n",
    "    score = model.evaluate(x_test, y_test, verbose=verbose, batch_size=batch_size)\n",
    "    loss = score[0]\n",
    "    accuracy = score[1]\n",
    "    #print('Test loss: {}'.format(loss))\n",
    "    #print('Test accuracy: {}'.format(accuracy))\n",
    "    return (model, loss, accuracy)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ApplyNet(input_data, model):\n",
    "    prediction = model.predict([input_data]) \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrcis(statistics_data, title:str, metric:str = 'accuarcy'):\n",
    "    \n",
    "    # Define labels, positions, bar heights and error bar heights\n",
    "    x_pos = np.arange(len(statistics_data.keys()))\n",
    "    \n",
    "    # Define labels, positions, bar heights and error bar heights\n",
    "    model_names = statistics_data.keys()\n",
    "    \n",
    "    plot_data_dict = {}\n",
    "\n",
    "    for model in statsDict.keys():\n",
    "        for data_source_key in statsDict[model].keys():\n",
    "            try:\n",
    "                plot_data_dict[data_source_key].append(statsDict[model][data_source_key][metric])\n",
    "            except KeyError:\n",
    "                plot_data_dict[data_source_key] = []\n",
    "                plot_data_dict[data_source_key].append(statsDict[model][data_source_key][metric])\n",
    "\n",
    "        \n",
    "    \n",
    "    # create plot\n",
    "    width = 0.2    \n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    \n",
    "    counter = 0\n",
    "    for label, data in plot_data_dict.items():\n",
    "        plt.bar(x_pos + width*counter, data, width, label=label)\n",
    "        counter += 1\n",
    "    \n",
    "    \n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xticks(x_pos + width)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.set_title(title)\n",
    "    ax.yaxis.grid(True)\n",
    "    \n",
    "    \n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "    plt.legend(loc='best')\n",
    "    # Save the figure and show\n",
    "    #save_fig(\"{}_metrics\".format(title.replace(\" \", \"_\")))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CNN...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_7_input to have 3 dimensions, but got array with shape (1008, 1200)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-f9119f834c19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#print(\"Online Frequency Data\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     model, loss, accuarcy = function(X_train_freq, y_train_freq, X_test_freq, y_test_freq,\n\u001b[1;32m---> 19\u001b[1;33m                                      epochs=epochs, batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mstatsDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnet_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'online_freq_data'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"model\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"accuarcy\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0maccuarcy\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-91bde855671e>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[1;34m(x_train, y_train, x_test, y_test, batch_size, num_classes, epochs, verbose)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;31m#    batch_size=100, epochs=40, validation_data=(x_val,y_val))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m#Evaluate neural net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv1d_7_input to have 3 dimensions, but got array with shape (1008, 1200)"
     ]
    }
   ],
   "source": [
    "netFunctionList = [ #(\"Easy Net\", trainEasyNet)];\n",
    "                   (\"CNN\", train_cnn)]\n",
    "                   #(\"LSTM Net\", train_lstm)]\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "verbose = 1\n",
    "num_classes = 2\n",
    "\n",
    "statsDict = {}\n",
    "\n",
    "for net_name, function in netFunctionList:\n",
    "    print(\"Evaluating {}...\".format(net_name))\n",
    "    \n",
    "    statsDict[net_name] = {}\n",
    "    \n",
    "    #print(\"Online Frequency Data\")\n",
    "    model, loss, accuarcy = function(X_train_freq, y_train_freq, X_test_freq, y_test_freq,\n",
    "                                     epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    statsDict[net_name]['online_freq_data'] = {\"model\" : model, \"loss\" : loss, \"accuarcy\" : accuarcy}\n",
    "    \n",
    "    #print(\"Online Entropy Data\")\n",
    "    model, loss, accuarcy = function(X_train_entropy, y_train_entropy, X_test_entropy, y_test_entropy,\n",
    "                                     epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    statsDict[net_name]['online_entropy_data'] = {\"model\" : model, \"loss\" : loss, \"accuarcy\" : accuarcy}\n",
    "    \n",
    "    #print(\"Experiement Frequency Data\")\n",
    "    model, loss, accuarcy = function(X_train_freq_exp, y_train_freq_exp, X_test_freq_exp, y_test_freq_exp,\n",
    "                                     epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    statsDict[net_name]['experiment_freq_data'] = {\"model\" : model, \"loss\" : loss, \"accuarcy\" : accuarcy}\n",
    "    \n",
    "    #print(\"Experiement Entropy Data\")\n",
    "    model, loss, accuarcy = function(X_train_entropy_exp, y_train_entropy_exp, X_test_entropy_exp, y_test_entropy_exp,\n",
    "                                     epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    statsDict[net_name]['experiment_entropy_data'] = {\"model\" : model, \"loss\" : loss, \"accuarcy\" : accuarcy}\n",
    "\n",
    "print(\"Done\")\n",
    "plot_metrcis(statsDict, title=\"Accuarcy comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1008, 1200)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_freq.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
