{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Masterthesis\\thesis_eeg\\code\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# to enable local imports\n",
    "module_path = os.path.abspath('../../code')\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from machine_learning_load_data import loadOnlineEEGdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some online EEG Data\n",
    "eegData, freqData, entropyData = loadOnlineEEGdata(dirPath='../../../EEG_Data/eeg_data_online', splitData=True)\n",
    "\n",
    "X_train_eeg, y_train_eeg, X_test_eeg, y_test_eeg = eegData\n",
    "X_train_freq, y_train_freq, X_test_freq, y_test_freq = freqData\n",
    "X_train_entropy, y_train_entropy, X_test_entropy, y_test_entropy = entropyData\n",
    "\n",
    "# reshape\n",
    "X_train_freq = X_train_freq.reshape(X_train_freq.shape[0], X_train_freq.shape[2])\n",
    "X_test_freq = X_test_freq.reshape(X_test_freq.shape[0], X_test_freq.shape[2])\n",
    "\n",
    "X_train_entropy = X_train_entropy.reshape(X_train_entropy.shape[0], X_train_entropy.shape[2])\n",
    "X_test_entropy = X_test_entropy.reshape(X_test_entropy.shape[0], X_test_entropy.shape[2])\n",
    "\n",
    "\n",
    "##################\n",
    "# experiment data\n",
    "##################\n",
    "eegData_exp, freqData_exp, entropyData_exp = loadOnlineEEGdata(dirPath='../../../EEG_Data/muse_data', splitData=True)\n",
    "\n",
    "X_train_eeg_exp, y_train_eeg_exp, X_test_eeg_exp, y_test_eeg_exp = eegData_exp\n",
    "X_train_freq_exp, y_train_freq_exp, X_test_freq_exp, y_test_freq_exp = freqData_exp\n",
    "X_train_entropy_exp, y_train_entropy_exp, X_test_entropy_exp, y_test_entropy_exp = entropyData_exp\n",
    "\n",
    "# reshape\n",
    "X_train_freq_exp = X_train_freq_exp.reshape(X_train_freq_exp.shape[0], X_train_freq_exp.shape[2])\n",
    "X_test_freq_exp = X_test_freq_exp.reshape(X_test_freq_exp.shape[0], X_test_freq_exp.shape[2])\n",
    "\n",
    "X_train_entropy_exp = X_train_entropy_exp.reshape(X_train_entropy_exp.shape[0], X_train_entropy_exp.shape[2])\n",
    "X_test_entropy_exp = X_test_entropy_exp.reshape(X_test_entropy_exp.shape[0], X_test_entropy_exp.shape[2])\n",
    "\n",
    "#targetLabelsDict_exp = loadTargetLabelsTxt(filePath='../../EEG_Data/muse_data/target_labels.txt')\n",
    "#targetNames_exp = ['AWAKE ({})'.format(targetLabelsDict_exp['AWAKE']),\n",
    "#               'FATIGUE ({})'.format(targetLabelsDict_exp['FATIGUE'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainEvaluateNet(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    #Convert data set to numpy array of type float32\n",
    "    #x_train = np.asarray(x_train, np.float32)\n",
    "    #y_train = np.asarray(y_train, np.float32)\n",
    "    #x_test = np.asarray(x_test, np.float32)\n",
    "    #y_test = np.asarray(y_test, np.float32)\n",
    "\n",
    "    #Number of neurons\n",
    "    neurons_num = int((2/3) * x_train.shape[2])\n",
    "\n",
    "    #Initialize neural net\n",
    "    #output of layer = keras.layers.Dense(number of neurons, activation function, name)(input of layer)\n",
    "    inputs = keras.Input(shape=(x_train.shape[1], x_train.shape[2]), name='input_points')\n",
    "    x = keras.layers.Dense(neurons_num, activation='relu', name='L1')(inputs)\n",
    "    x = keras.layers.Dense(neurons_num, activation='relu', name='L2')(x)\n",
    "    outputs = keras.layers.Dense(2, activation='softmax', name='output_point')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='fatigue_classifier')\n",
    "\n",
    "    #Set optimizer, loss function and metrics to track\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    #binary_crossentropy\n",
    "\n",
    "    #Summary of the neural net\n",
    "    #model.summary()\n",
    "    \n",
    "    #Train neural net\n",
    "    model.fit(x_train, y_train, epochs=3)\n",
    "\n",
    "    #Evaluate neural net\n",
    "    score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "    loss = score[0]\n",
    "    accuracy = score[1]\n",
    "    print('Test loss:', loss)\n",
    "    print('Test accuracy:', accuracy)\n",
    "    return (model, loss, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEvaluateNet(x_train, y_train, x_test, y_test,batch_size=64, num_classes=2, epochs=5, verbose=0):\n",
    "\n",
    "    #Settings\n",
    "    neurons_num = int((2/3) * x_train.shape[2]) #Number of neurons\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    #Initialize neural net\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=10, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(Conv1D(64, kernel_size=1, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neurons_num, activation='relu'))\n",
    "    model.add(Dense(neurons_num, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax', name='last_dense')) \n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    #Set optimizer, loss function and metrics to track\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(0.01), metrics=['accuracy'])\n",
    "    #model.compile(loss=keras.losses.squared_hinge, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "    #Summary of the neural net\n",
    "    #model.summary()\n",
    "    \n",
    "    #Train neural net\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=verbose,\n",
    "              validation_data=(x_test, y_test))\n",
    "    #         validation_split=0.2)\n",
    "\n",
    "    #Evaluate neural net\n",
    "    score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "    loss = score[0]\n",
    "    accuracy = score[1]\n",
    "    print('Test loss: {}'.format(loss))\n",
    "    print('Test accuracy: {}'.format(accuracy))\n",
    "    return (model, loss, accuracy)\n",
    "\n",
    "    #Predict best route\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEasyNet(x_train, y_train, x_test, y_test, batch_size=64, num_classes=2, epochs=5, verbose=0):\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    neurons_num = int((2/3) * x_train.shape[1]) #Number of neurons\n",
    "    \n",
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(neurons_num*2), input_dim=x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(neurons_num, activation='relu'))\n",
    "    #model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(int(neurons_num/2), activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax', name='last_dense'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    # compile the keras model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(0.01), metrics=['accuracy'])\n",
    "    # fit the keras model on the dataset\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    #Evaluate neural net\n",
    "    score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "    loss = score[0]\n",
    "    accuracy = score[1]\n",
    "    print('Test loss: {}'.format(loss))\n",
    "    print('Test accuracy: {}'.format(accuracy))\n",
    "    return (model, loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def train_lstm(x_train, y_train, x_test, y_test, batch_size=64, num_classes=2, epochs=5, verbose=0):\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    try:\n",
    "        n_timesteps, n_features, n_outputs = x_train.shape[1], x_train.shape[2], y_train.shape[1]\n",
    "    except IndexError: \n",
    "        # reshape that...\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "        n_timesteps, n_features, n_outputs = x_train.shape[1], x_train.shape[2], y_train.shape[1]\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(\"n_timesteps: {} - n_features: {} - n_outputs: {}\".format(n_timesteps, n_features, n_outputs))\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    #_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    #Evaluate neural net\n",
    "    score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "    loss = score[0]\n",
    "    accuracy = score[1]\n",
    "    print('Test loss: {}'.format(loss))\n",
    "    print('Test accuracy: {}'.format(accuracy))\n",
    "    return (model, loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyNet(input_data, model):\n",
    "    prediction = model.predict([input_data]) \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrcis(statistics_data, title:str):\n",
    "    \n",
    "    # Define labels, positions, bar heights and error bar heights\n",
    "    x_pos = np.arange(len(statistics_data.keys()))\n",
    "    \n",
    "    # create plot\n",
    "    width = 0.2    \n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    \n",
    "    plot_data = {}\n",
    "    for key in statistics_data.keys():\n",
    "        \n",
    "        for label in statistics_data[key].keys():\n",
    "            \n",
    "            try:\n",
    "                plot_data[label].append((statistics_data[key][label]['loss'],\n",
    "                                        statistics_data[key][label]['accuarcy']))\n",
    "                \n",
    "            except Exception as e:\n",
    "                plot_data[label] = []\n",
    "                plot_data[label].append((statistics_data[key][label]['loss'],\n",
    "                                        statistics_data[key][label]['accuarcy']))\n",
    "        \n",
    "    \n",
    "    for label in \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'online_freq_data': [(1.1378809221916728, 0.5902777910232544), (1.3613783563659698, 0.5694444179534912)], 'online_entropy_data': [(1.4110502578964943, 0.5589600801467896), (1.2214424970145565, 0.5366759300231934)], 'experiment_freq_data': [(1.6079379791133506, 0.6387096643447876), (1.1481416922743601, 0.6279569864273071)], 'experiment_entropy_data': [(0.6988416079006393, 0.5778546929359436), (0.7456444809181055, 0.5640138387680054)]}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAAEzCAYAAADHIU4yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPtUlEQVR4nO3dX4ild33H8c+3uwr+q4rZis0fDCUaU0iKjtGLirHSmnjRIFhIFKVBWEKNeJlc6YU39UIQMbosEoI35qIGjSUaeqMWNDQT0GiUyBJpso2QjYoFhYbVby9mLNPxfDMnkzNn1s3rBQP7PM9vZr4wP2bfPHv2PNXdAQAA/tCfHPYAAABwrhLLAAAwEMsAADAQywAAMBDLAAAwEMsAADDYM5ar6o6qerKqfjhcr6r6TFWdqqqHquqNqx8TAADWb5k7y3cmufYZrl+X5LLtj+NJPv/cxwIAgMO3Zyx397eT/OIZllyf5Iu95f4kr6iq16xqQAAAOCyreM3yhUke33F8evscAAD8UTu6gq9RC84tfIZ2VR3P1ks18pKXvORNl19++Qq+PQAAzB588MGnuvvYfj53FbF8OsnFO44vSvLEooXdfTLJySTZ2Njozc3NFXx7AACYVdV/7vdzV/EyjHuSfHD7XTHemuRX3f2zFXxdAAA4VHveWa6qLyW5JskFVXU6yceTvCBJuvtEknuTvDvJqSS/SXLTQQ0LAADrtGcsd/eNe1zvJB9e2UQAAHCO8AQ/AAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYiGUAABiIZQAAGIhlAAAYLBXLVXVtVT1SVaeq6rYF119eVV+rqu9X1cNVddPqRwUAgPXaM5ar6kiS25Ncl+SKJDdW1RW7ln04yY+6+6ok1yT5VFW9cMWzAgDAWi1zZ/nqJKe6+9HufjrJXUmu37Wmk7ysqirJS5P8IsnZlU4KAABrtkwsX5jk8R3Hp7fP7fTZJG9I8kSSHyT5aHf/bvcXqqrjVbVZVZtnzpzZ58gAALAey8RyLTjXu47fleR7Sf48yV8l+WxV/ekffFL3ye7e6O6NY8eOPethAQBgnZaJ5dNJLt5xfFG27iDvdFOSu3vLqSQ/TXL5akYEAIDDsUwsP5Dksqq6dPs/7d2Q5J5dax5L8s4kqapXJ3l9kkdXOSgAAKzb0b0WdPfZqrolyX1JjiS5o7sfrqqbt6+fSPKJJHdW1Q+y9bKNW7v7qQOcGwAADtyesZwk3X1vknt3nTux489PJPm71Y4GAACHyxP8AABgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYLBULFfVtVX1SFWdqqrbhjXXVNX3qurhqvrWascEAID1O7rXgqo6kuT2JH+b5HSSB6rqnu7+0Y41r0jyuSTXdvdjVfVnBzUwAACsyzJ3lq9Ocqq7H+3up5PcleT6XWvel+Tu7n4sSbr7ydWOCQAA67dMLF+Y5PEdx6e3z+30uiSvrKpvVtWDVfXBVQ0IAACHZc+XYSSpBed6wdd5U5J3JnlRku9W1f3d/ZP/94Wqjic5niSXXHLJs58WAADWaJk7y6eTXLzj+KIkTyxY843u/nV3P5Xk20mu2v2Fuvtkd29098axY8f2OzMAAKzFMrH8QJLLqurSqnphkhuS3LNrzVeTvK2qjlbVi5O8JcmPVzsqAACs154vw+jus1V1S5L7khxJckd3P1xVN29fP9HdP66qbyR5KMnvknyhu394kIMDAMBBq+7dLz9ej42Njd7c3DyU7w0AwPNHVT3Y3Rv7+VxP8AMAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAICBWAYAgIFYBgCAgVgGAIDBUrFcVddW1SNVdaqqbnuGdW+uqt9W1XtXNyIAAByOPWO5qo4kuT3JdUmuSHJjVV0xrPtkkvtWPSQAAByGZe4sX53kVHc/2t1PJ7kryfUL1n0kyZeTPLnC+QAA4NAsE8sXJnl8x/Hp7XP/p6ouTPKeJCdWNxoAAByuZWK5FpzrXcefTnJrd//2Gb9Q1fGq2qyqzTNnziw7IwAAHIqjS6w5neTiHccXJXli15qNJHdVVZJckOTdVXW2u7+yc1F3n0xyMkk2NjZ2BzcAAJxTlonlB5JcVlWXJvmvJDcked/OBd196e//XFV3JvnX3aEMAAB/bPaM5e4+W1W3ZOtdLo4kuaO7H66qm7eve50yAADnpWXuLKe7701y765zCyO5u//xuY8FAACHzxP8AABgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgIJYBAGAglgEAYCCWAQBgsFQsV9W1VfVIVZ2qqtsWXH9/VT20/fGdqrpq9aMCAMB67RnLVXUkye1JrktyRZIbq+qKXct+muTt3X1lkk8kObnqQQEAYN2WubN8dZJT3f1odz+d5K4k1+9c0N3f6e5fbh/en+Si1Y4JAADrt0wsX5jk8R3Hp7fPTT6U5OuLLlTV8ararKrNM2fOLD8lAAAcgmViuRac64ULq96RrVi+ddH17j7Z3RvdvXHs2LHlpwQAgENwdIk1p5NcvOP4oiRP7F5UVVcm+UKS67r756sZDwAADs8yd5YfSHJZVV1aVS9MckOSe3YuqKpLktyd5APd/ZPVjwkAAOu3553l7j5bVbckuS/JkSR3dPfDVXXz9vUTST6W5FVJPldVSXK2uzcObmwAADh41b3w5ccHbmNjozc3Nw/lewMA8PxRVQ/u90auJ/gBAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAQCwDAMBALAMAwEAsAwDAYKlYrqprq+qRqjpVVbctuF5V9Znt6w9V1RtXPyoAAKzXnrFcVUeS3J7kuiRXJLmxqq7Ytey6JJdtfxxP8vkVzwkAAGu3zJ3lq5Oc6u5Hu/vpJHcluX7XmuuTfLG33J/kFVX1mhXPCgAAa7VMLF+Y5PEdx6e3zz3bNQAA8Efl6BJrasG53seaVNXxbL1MI0n+p6p+uMT35/nlgiRPHfYQnHPsCxaxL1jEvmCR1+/3E5eJ5dNJLt5xfFGSJ/axJt19MsnJJKmqze7eeFbTct6zL1jEvmAR+4JF7AsWqarN/X7uMi/DeCDJZVV1aVW9MMkNSe7ZteaeJB/cfleMtyb5VXf/bL9DAQDAuWDPO8vdfbaqbklyX5IjSe7o7oer6ubt6yeS3Jvk3UlOJflNkpsObmQAAFiPZV6Gke6+N1tBvPPciR1/7iQffpbf++SzXM/zg33BIvYFi9gXLGJfsMi+90VtdS4AALCbx10DAMDgwGPZo7JZZIl98f7t/fBQVX2nqq46jDlZr732xY51b66q31bVe9c5H4djmX1RVddU1feq6uGq+ta6Z2T9lvh75OVV9bWq+v72vvD/qc5zVXVHVT05vTXxfpvzQGPZo7JZZMl98dMkb+/uK5N8Il6Ddt5bcl/8ft0ns/WfjjnPLbMvquoVST6X5O+7+y+T/MPaB2Wtlvx98eEkP+ruq5Jck+RT2+/qxfnrziTXPsP1fTXnQd9Z9qhsFtlzX3T3d7r7l9uH92frvbs5vy3z+yJJPpLky0meXOdwHJpl9sX7ktzd3Y8lSXfbG+e/ZfZFJ3lZVVWSlyb5RZKz6x2Tderub2fr5zzZV3MedCx7VDaLPNuf+YeSfP1AJ+JcsOe+qKoLk7wnyYnwfLHM74vXJXllVX2zqh6sqg+ubToOyzL74rNJ3pCth6T9IMlHu/t36xmPc9S+mnOpt457Dlb2qGzOK0v/zKvqHdmK5b8+0Ik4FyyzLz6d5Nbu/u3WzSKeB5bZF0eTvCnJO5O8KMl3q+r+7v7JQQ/HoVlmX7wryfeS/E2Sv0jyb1X179393wc9HOesfTXnQcfyyh6VzXllqZ95VV2Z5AtJruvun69pNg7PMvtiI8ld26F8QZJ3V9XZ7v7KekbkECz798hT3f3rJL+uqm8nuSqJWD5/LbMvbkryz9vPgjhVVT9NcnmS/1jPiJyD9tWcB/0yDI/KZpE990VVXZLk7iQfcHfoeWPPfdHdl3b3a7v7tUn+Jck/CeXz3jJ/j3w1yduq6mhVvTjJW5L8eM1zsl7L7IvHsvWvDamqVyd5fZJH1zol55p9NeeB3ln2qGwWWXJffCzJq5J8bvsu4tnu3jismTl4S+4LnmeW2Rfd/eOq+kaSh5L8LskXunvhW0dxfljy98UnktxZVT/I1j+/39rdTx3a0By4qvpStt755IKqOp3k40lekDy35vQEPwAAGHiCHwAADMQyAAAMxDIAAAzEMgAADMQyAAAMxDIAAAzEMgAADMQyAAAM/hdgIckV/Tn6pgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrcis(statsDict, title=\"asd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Easy Net\n",
      "Online Frequency Data\n",
      "Test loss: 1.1378809221916728\n",
      "Test accuracy: 0.5902777910232544\n",
      "Test loss: 1.4110502578964943\n",
      "Test accuracy: 0.5589600801467896\n",
      "Experiement Frequency Data\n",
      "Test loss: 1.6079379791133506\n",
      "Test accuracy: 0.6387096643447876\n",
      "Experiement Entropy Data\n",
      "Test loss: 0.6988416079006393\n",
      "Test accuracy: 0.5778546929359436\n",
      "Evaluating LSTM Net\n",
      "Online Frequency Data\n",
      "Test loss: 1.3613783563659698\n",
      "Test accuracy: 0.5694444179534912\n",
      "Test loss: 1.2214424970145565\n",
      "Test accuracy: 0.5366759300231934\n",
      "Experiement Frequency Data\n",
      "Test loss: 1.1481416922743601\n",
      "Test accuracy: 0.6279569864273071\n",
      "Experiement Entropy Data\n",
      "Test loss: 0.7456444809181055\n",
      "Test accuracy: 0.5640138387680054\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "netFunctionList = [(\"Easy Net\", trainEasyNet), (\"LSTM Net\", train_lstm)]\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 64\n",
    "verbose = 0\n",
    "num_classes = 2\n",
    "\n",
    "statsDict = {}\n",
    "\n",
    "for net_name, function in netFunctionList:\n",
    "    print(\"Evaluating {}\".format(net_name))\n",
    "    \n",
    "    statsDict[net_name] = {}\n",
    "    \n",
    "    print(\"Online Frequency Data\")\n",
    "    model, loss, accuarcy = function(X_train_freq, y_train_freq, X_test_freq, y_test_freq,\n",
    "                                     epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    statsDict[net_name]['online_freq_data'] = {\"model\" : model, \"loss\" : loss, \"accuarcy\" : accuarcy}\n",
    "    \n",
    "    \n",
    "    model, loss, accuarcy = function(X_train_entropy, y_train_entropy, X_test_entropy, y_test_entropy,\n",
    "                                     epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    statsDict[net_name]['online_entropy_data'] = {\"model\" : model, \"loss\" : loss, \"accuarcy\" : accuarcy}\n",
    "    \n",
    "    print(\"Experiement Frequency Data\")\n",
    "    model, loss, accuarcy = function(X_train_freq_exp, y_train_freq_exp, X_test_freq_exp, y_test_freq_exp,\n",
    "                                     epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    statsDict[net_name]['experiment_freq_data'] = {\"model\" : model, \"loss\" : loss, \"accuarcy\" : accuarcy}\n",
    "    \n",
    "    print(\"Experiement Entropy Data\")\n",
    "    model, loss, accuarcy = function(X_train_entropy_exp, y_train_entropy_exp, X_test_entropy_exp, y_test_entropy_exp,\n",
    "                                     epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    statsDict[net_name]['experiment_entropy_data'] = {\"model\" : model, \"loss\" : loss, \"accuarcy\" : accuarcy}\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'online_freq_data': {'model': <keras.engine.sequential.Sequential at 0x258e5f60688>,\n",
       "  'loss': 1.1378809221916728,\n",
       "  'accuarcy': 0.5902777910232544},\n",
       " 'online_entropy_data': {'model': <keras.engine.sequential.Sequential at 0x258d362fa88>,\n",
       "  'loss': 1.4110502578964943,\n",
       "  'accuarcy': 0.5589600801467896},\n",
       " 'experiment_freq_data': {'model': <keras.engine.sequential.Sequential at 0x258cf176fc8>,\n",
       "  'loss': 1.6079379791133506,\n",
       "  'accuarcy': 0.6387096643447876},\n",
       " 'experiment_entropy_data': {'model': <keras.engine.sequential.Sequential at 0x258d3e5ad88>,\n",
       "  'loss': 0.6988416079006393,\n",
       "  'accuarcy': 0.5778546929359436}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statsDict['Easy Net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1008/1008 [==============================] - 0s 471us/step - loss: 3.2995 - accuracy: 0.5258\n",
      "Epoch 2/10\n",
      "1008/1008 [==============================] - 0s 357us/step - loss: 0.6450 - accuracy: 0.6230\n",
      "Epoch 3/10\n",
      "1008/1008 [==============================] - 0s 330us/step - loss: 0.5557 - accuracy: 0.7222\n",
      "Epoch 4/10\n",
      "1008/1008 [==============================] - 0s 333us/step - loss: 0.4948 - accuracy: 0.7788\n",
      "Epoch 5/10\n",
      "1008/1008 [==============================] - 0s 337us/step - loss: 0.4586 - accuracy: 0.7956\n",
      "Epoch 6/10\n",
      "1008/1008 [==============================] - 0s 342us/step - loss: 0.3816 - accuracy: 0.8313\n",
      "Epoch 7/10\n",
      "1008/1008 [==============================] - 0s 347us/step - loss: 0.3965 - accuracy: 0.8264\n",
      "Epoch 8/10\n",
      "1008/1008 [==============================] - 0s 333us/step - loss: 0.2886 - accuracy: 0.8730\n",
      "Epoch 9/10\n",
      "1008/1008 [==============================] - 0s 337us/step - loss: 0.3082 - accuracy: 0.8532\n",
      "Epoch 10/10\n",
      "1008/1008 [==============================] - 0s 372us/step - loss: 0.2579 - accuracy: 0.8919\n",
      "Test loss: 1.0882309201966833\n",
      "Test accuracy: 0.6041666865348816\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
