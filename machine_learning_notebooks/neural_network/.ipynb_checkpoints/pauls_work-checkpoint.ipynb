{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Masterthesis\\thesis_eeg\\code\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "\n",
    "\n",
    "# to enable local imports\n",
    "module_path = os.path.abspath('../../code')\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from machine_learning_load_data import loadOnlineEEGdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Online EEG Data from ../../../EEG_Data/eeg_data_online ...\n",
      "EEG Data Shape:\n",
      "(5024, 512, 40) (5024,) (2154, 512, 40) (2154,)\n",
      "Freq Data Shape:\n",
      "(1008, 1, 1200) (1008,) (432, 1, 1200) (432,)\n",
      "Entropy Data Shape:\n",
      "(5024, 1, 200) (5024,) (2154, 1, 200) (2154,)\n"
     ]
    }
   ],
   "source": [
    "# Load some online EEG Data\n",
    "eegData, freqData, entropyData = loadOnlineEEGdata(dirPath='../../../EEG_Data/eeg_data_online', splitData=True)\n",
    "\n",
    "X_train_eeg, y_train_eeg, X_test_eeg, y_test_eeg = eegData\n",
    "X_train_freq, y_train_freq, X_test_freq, y_test_freq = freqData\n",
    "X_train_entropy, y_train_entropy, X_test_entropy, y_test_entropy = entropyData\n",
    "\n",
    "# reshape\n",
    "#X_train_freq = X_train_freq.reshape(X_train_freq.shape[0], X_train_freq.shape[2])\n",
    "#X_test_freq = X_test_freq.reshape(X_test_freq.shape[0], X_test_freq.shape[2])\n",
    "\n",
    "#X_train_entropy = X_train_entropy.reshape(X_train_entropy.shape[0], X_train_entropy.shape[2])\n",
    "#X_test_entropy = X_test_entropy.reshape(X_test_entropy.shape[0], X_test_entropy.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainEvaluateNet(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    #Convert data set to numpy array of type float32\n",
    "    #x_train = np.asarray(x_train, np.float32)\n",
    "    #y_train = np.asarray(y_train, np.float32)\n",
    "    #x_test = np.asarray(x_test, np.float32)\n",
    "    #y_test = np.asarray(y_test, np.float32)\n",
    "\n",
    "    #Number of neurons\n",
    "    neurons_num = int((2/3) * x_train.shape[2])\n",
    "\n",
    "    #Initialize neural net\n",
    "    #output of layer = keras.layers.Dense(number of neurons, activation function, name)(input of layer)\n",
    "    inputs = keras.Input(shape=(x_train.shape[1], x_train.shape[2]), name='input_points')\n",
    "    x = keras.layers.Dense(neurons_num, activation='relu', name='L1')(inputs)\n",
    "    x = keras.layers.Dense(neurons_num, activation='relu', name='L2')(x)\n",
    "    outputs = keras.layers.Dense(2, activation='softmax', name='output_point')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='fatigue_classifier')\n",
    "\n",
    "    #Set optimizer, loss function and metrics to track\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    #binary_crossentropy\n",
    "\n",
    "    #Summary of the neural net\n",
    "    #model.summary()\n",
    "    \n",
    "    #Train neural net\n",
    "    model.fit(x_train, y_train, epochs=3)\n",
    "\n",
    "    #Evaluate neural net\n",
    "    model.evaluate(x_test, y_test)\n",
    "\n",
    "    #Predict best route\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1008, 40, 30, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_conv = convertDataSet(X_train_freq, channels=30)\n",
    "freq_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDataSet(dataset, channels):\n",
    "    #converts the frequency dataset from (1008,1,1200) (samples, placeholder, channels + values) in (1008,40,30,1) (samples, channels, values, placeholder)\n",
    "    newdataset = np.zeros((int(dataset.shape[0]), int((dataset.shape[2]/channels)), int(channels), 1))\n",
    "    for sample in range(dataset.shape[0]):\n",
    "        for channel in range(0, dataset.shape[2], channels):\n",
    "            for values in range(channels):\n",
    "                newdataset[sample, int(channel/30), values, 0] = dataset[sample, 0, channel + values]\n",
    "    \n",
    "    newdataset = np.asarray(newdataset, np.float32)\n",
    "    return newdataset\n",
    "    \n",
    "def trainEvaluateNet(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    #Settings\n",
    "    batch_size = 64\n",
    "    num_classes = 2\n",
    "    epochs = 20\n",
    "    neurons_num = int((2/3) * x_train.shape[2]) #Number of neurons\n",
    "\n",
    "    #converting training dataset\n",
    "    #x_train = convertDataSet(x_train, 30)\n",
    "    #x_test = convertDataSet(x_test, 30)\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    #Initialize neural net\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=10, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(Conv1D(64, kernel_size=1, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neurons_num, activation='relu'))\n",
    "    model.add(Dense(neurons_num, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax', name='last_dense')) \n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    #Set optimizer, loss function and metrics to track\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(0.01), metrics=['accuracy'])\n",
    "    #model.compile(loss=keras.losses.squared_hinge, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "    #Summary of the neural net\n",
    "    #model.summary()\n",
    "    \n",
    "    #Train neural net\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    #         validation_split=0.2)\n",
    "\n",
    "    #Evaluate neural net\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    #Predict best route\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEasyNet(x_train, y_train, x_test, y_test, batch_size=64, num_classes=2, epochs=5):\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile the keras model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit the keras model on the dataset\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    #Evaluate neural net\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 1, 32)             38432     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 1, 64)             2112      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 800)               52000     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 800)               640800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "last_dense (Dense)           (None, 2)                 1602      \n",
      "=================================================================\n",
      "Total params: 734,946\n",
      "Trainable params: 734,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1008 samples, validate on 432 samples\n",
      "Epoch 1/20\n",
      "1008/1008 [==============================] - 1s 525us/step - loss: 1.0542 - accuracy: 0.4980 - val_loss: 0.6948 - val_accuracy: 0.4468\n",
      "Epoch 2/20\n",
      "1008/1008 [==============================] - 0s 258us/step - loss: 0.6894 - accuracy: 0.5843 - val_loss: 0.7744 - val_accuracy: 0.4444\n",
      "Epoch 3/20\n",
      "1008/1008 [==============================] - 0s 266us/step - loss: 0.6990 - accuracy: 0.5456 - val_loss: 0.6724 - val_accuracy: 0.5995\n",
      "Epoch 4/20\n",
      "1008/1008 [==============================] - 0s 256us/step - loss: 0.6584 - accuracy: 0.6091 - val_loss: 0.6554 - val_accuracy: 0.5718\n",
      "Epoch 5/20\n",
      "1008/1008 [==============================] - 0s 261us/step - loss: 0.6194 - accuracy: 0.6567 - val_loss: 0.7214 - val_accuracy: 0.5810\n",
      "Epoch 6/20\n",
      "1008/1008 [==============================] - 0s 257us/step - loss: 0.5518 - accuracy: 0.7232 - val_loss: 0.7381 - val_accuracy: 0.5255\n",
      "Epoch 7/20\n",
      "1008/1008 [==============================] - 0s 253us/step - loss: 0.5337 - accuracy: 0.7540 - val_loss: 0.7999 - val_accuracy: 0.5440\n",
      "Epoch 8/20\n",
      "1008/1008 [==============================] - 0s 278us/step - loss: 0.5167 - accuracy: 0.7550 - val_loss: 0.7384 - val_accuracy: 0.5694\n",
      "Epoch 9/20\n",
      "1008/1008 [==============================] - 0s 282us/step - loss: 0.4763 - accuracy: 0.7927 - val_loss: 0.9081 - val_accuracy: 0.5486\n",
      "Epoch 10/20\n",
      "1008/1008 [==============================] - 0s 252us/step - loss: 0.4000 - accuracy: 0.8353 - val_loss: 1.2374 - val_accuracy: 0.5972\n",
      "Epoch 11/20\n",
      "1008/1008 [==============================] - 0s 251us/step - loss: 0.3804 - accuracy: 0.8433 - val_loss: 0.8839 - val_accuracy: 0.6042\n",
      "Epoch 12/20\n",
      "1008/1008 [==============================] - 0s 247us/step - loss: 0.4137 - accuracy: 0.8234 - val_loss: 1.1336 - val_accuracy: 0.5810\n",
      "Epoch 13/20\n",
      "1008/1008 [==============================] - 0s 250us/step - loss: 0.4020 - accuracy: 0.8304 - val_loss: 0.9211 - val_accuracy: 0.5764\n",
      "Epoch 14/20\n",
      "1008/1008 [==============================] - 0s 252us/step - loss: 0.3400 - accuracy: 0.8601 - val_loss: 0.9679 - val_accuracy: 0.5810\n",
      "Epoch 15/20\n",
      "1008/1008 [==============================] - 0s 266us/step - loss: 0.3055 - accuracy: 0.8770 - val_loss: 1.2913 - val_accuracy: 0.5833\n",
      "Epoch 16/20\n",
      "1008/1008 [==============================] - 0s 256us/step - loss: 0.2642 - accuracy: 0.9038 - val_loss: 1.3044 - val_accuracy: 0.5671\n",
      "Epoch 17/20\n",
      "1008/1008 [==============================] - 0s 264us/step - loss: 0.3229 - accuracy: 0.8700 - val_loss: 0.9269 - val_accuracy: 0.5694\n",
      "Epoch 18/20\n",
      "1008/1008 [==============================] - 0s 263us/step - loss: 0.2765 - accuracy: 0.8988 - val_loss: 1.4526 - val_accuracy: 0.5880\n",
      "Epoch 19/20\n",
      "1008/1008 [==============================] - 0s 258us/step - loss: 0.2410 - accuracy: 0.9226 - val_loss: 1.5596 - val_accuracy: 0.5671\n",
      "Epoch 20/20\n",
      "1008/1008 [==============================] - 0s 264us/step - loss: 0.2771 - accuracy: 0.8958 - val_loss: 1.0886 - val_accuracy: 0.5648\n",
      "Test loss: 1.0885710247136928\n",
      "Test accuracy: 0.5648148059844971\n"
     ]
    }
   ],
   "source": [
    "def ApplyNet(input_data, model):\n",
    "    prediction = model.predict([input_data]) \n",
    "    return prediction\n",
    "\n",
    "model = trainEvaluateNet(X_train_freq, y_train_freq, X_test_freq, y_test_freq)\n",
    "#ApplyNet(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = TrainEvaluateNet(X_train_entropy, y_train_entropy, X_test_entropy, y_test_entropy)\n",
    "#ApplyNet(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
