{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook shows how a raw Dataset gets processed for further machine learning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "# to enable local imports\n",
    "module_path = os.path.abspath(os.path.join('code'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import readFileCSV, loadConfigFile\n",
    "from pipelines import (filter_signal, pre_process_signal, feature_extraction, convert_data)\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def saveFeatureListToFile(featureList : List, filepath : str):\n",
    "    \n",
    "    if type(featureList) is not list:\n",
    "        raise Exception(\"The given feature list is not a list!\")\n",
    "    \n",
    "    print(\"Saving a feature list to: '{}'\".format(filepath))\n",
    "    \n",
    "    f = open(filepath, \"w\")\n",
    "    for feature in featureList:\n",
    "        line = \"{}\\n\".format(feature)\n",
    "        f.write(line)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "def processRawFileWithPipeline(filepath : str, yamlConfig) -> (pd.Series, pd.DataFrame, List[str]):\n",
    "    ''' Process a given filepath with the current pipelines \n",
    "    \n",
    "    This creates two different data objects:\n",
    "        epochSeries: This is a panda.Series which contains dataframes. Each index at the series represens one epoch\n",
    "        frequencyFreatureDf: This is a dataframe of the frequency features of the epochSeries. The index represnts the epochs. The features are the columns\n",
    "    '''\n",
    "    if not os.path.isfile(filepath):\n",
    "        raise Exception(\"The file '{}' does not exists!\".format(filepath))\n",
    "    \n",
    "    df = readFileCSV(filepath)\n",
    "    df, channelNameList =  convert_data(df=df, config=yamlConfig, starttime=None)\n",
    "    df = filter_signal(df=df, config=yamlConfig) # general filtering\n",
    "    epochSeries = pre_process_signal(df=df, config=yamlConfig)   # pre-processing\n",
    "    frequencyFeatureDf = feature_extraction(epochSeries=epochSeries, config=yamlConfig) # extract features\n",
    "    \n",
    "    return epochSeries, frequencyFeatureDf, channelNameList\n",
    "\n",
    "def safeAndProcessRawFileWithPipeline(rawFilePath : str, fileDir : str, label : str, yamlConfig):\n",
    "    ''' Process the given rawfilePath and safe the result as pickle files\n",
    "    This function calls 'processRawFileWithPipeline()' and the two returning data objects will be safed\n",
    "    \n",
    "    @param str rawFilePath: path to file which gets process\n",
    "    @param str fileDir: Directory where the data objects should be stored\n",
    "    @param str label: A label to know which data we process, e.g. fatigue, normal or awake data\n",
    "    @param yamlConfig: A loaded yaml config file for processing the data\n",
    "    '''\n",
    "    print (\"Starting to process {}...\".format(rawFilePath))\n",
    "    # process the file\n",
    "    epochSeries, frequencyFeatureDf, channelNameList = processRawFileWithPipeline(filepath=rawFilePath, yamlConfig=yamlConfig)\n",
    "    \n",
    "    # save the epoch series\n",
    "    epochSeries.to_pickle(os.path.join(fileDir,'epochSeries_{}.pkl'.format(label)))\n",
    "    \n",
    "    # save the frequency df\n",
    "    frequencyFeatureDf.to_pickle(os.path.join(fileDir,'frequencyFeaturesDf_{}.pkl'.format(label)))\n",
    "    \n",
    "    # save the channel name list\n",
    "    saveFeatureListToFile(featureList=channelNameList,\n",
    "                          filepath=os.path.join(fileDir, \"features_channel_names.txt\"))\n",
    "    \n",
    "    # save frequency features\n",
    "    saveFeatureListToFile(featureList=list(frequencyFeatureDf.columns),\n",
    "                          filepath=os.path.join(fileDir, \"features_frequency_df.txt\"))\n",
    "\n",
    "def processRawDatasetToPickleFiles(datasetDirPath : str, device : str, awakeFileName : str,\n",
    "                                   fatigueFileName : str, normalFileName : str, unlabeledFileName : str):\n",
    "    '''\n",
    "    @param str datasetDirPath: Path where the directory of the dataset is\n",
    "    @param str device: name of the device, to load the correct yaml file for processing\n",
    "    \n",
    "    Depending on the dataset there might be awake, normal, fatigue or unlabeled data. \n",
    "    @param awakeFileName: filename of the awake data or None then it will be ignored\n",
    "    @param fatigueFileName: filename of the fatigue data or None then it will be ignored\n",
    "    @param normalFileName: filename of the normal data or None then it will be ignored\n",
    "    @param unlabeledFileName: filename of the unlabeled data or None then it will be ignored\n",
    "    '''\n",
    "    \n",
    "    if not os.path.isdir(datasetDirPath):\n",
    "        raise Exception(\"The given dir path '{}' does not exist!\".format(datasetDirPath))\n",
    "        \n",
    "    # Load the yaml config file for the processing\n",
    "    yamlConfig = loadConfigFile(device)\n",
    "    \n",
    "    for root, dirs, files in os.walk(datasetDirPath):\n",
    "        for subjectDir in dirs:\n",
    "            print(\"#############################################\")\n",
    "            print(\"Process Subject {} Data...\".format(subjectDir))\n",
    "            print(\"---------------------------------------------\")\n",
    "            \n",
    "            if awakeFileName is not None: \n",
    "                safeAndProcessRawFileWithPipeline(rawFilePath=os.path.join(root, subjectDir, awakeFilename),\n",
    "                                                  fileDir=os.path.join(root, subjectDir),\n",
    "                                                  label = \"awake\",\n",
    "                                                  yamlConfig=yamlConfig)\n",
    "                \n",
    "            if fatigueFileName is not None: \n",
    "                safeAndProcessRawFileWithPipeline(rawFilePath=os.path.join(root, subjectDir, fatigueFileName),\n",
    "                                                  fileDir=os.path.join(root, subjectDir),\n",
    "                                                  label = \"fatigue\",\n",
    "                                                  yamlConfig=yamlConfig)\n",
    "                \n",
    "            if normalFileName is not None: \n",
    "                safeAndProcessRawFileWithPipeline(rawFilePath=os.path.join(root, subjectDir, normalFileName),\n",
    "                                                  fileDir=os.path.join(root, subjectDir),\n",
    "                                                  label = \"normal\",\n",
    "                                                  yamlConfig=yamlConfig)\n",
    "                \n",
    "            if unlabeledFileName is not None: \n",
    "                safeAndProcessRawFileWithPipeline(rawFilePath=os.path.join(root, subjectDir, normalFileName),\n",
    "                                                  fileDir=os.path.join(root, subjectDir),\n",
    "                                                  label = \"unlabeled\",\n",
    "                                                  yamlConfig=yamlConfig)\n",
    "    \n",
    "    print(\"#######################################\")\n",
    "    print(\"Done processing and saving a complete Dataset!\")\n",
    "\n",
    "def loadPickeldData(dataDir : str, label : str):\n",
    "    ''' Load the epochseries and frequency feature df\n",
    "    \n",
    "    @param str dataDir: Directory where the data is\n",
    "    @param str label: decide which \n",
    "    '''\n",
    "    try:\n",
    "        epochSeries = pd.read_pickle(os.path.join(dataDir,'epochSeries_{}.pkl'.format(label)))\n",
    "    except Exception as e:\n",
    "        #print (e)\n",
    "        epochSeries = None\n",
    "        \n",
    "    try:\n",
    "        frequencyFeatureDf = pd.read_pickle(os.path.join(dataDir,'frequencyFeaturesDf_{}.pkl'.format(label)))\n",
    "    except Exception as e:\n",
    "        #print (e)\n",
    "        frequencyFeatureDf = None\n",
    "\n",
    "    return epochSeries, frequencyFeatureDf\n",
    "\n",
    "def loadPickeldDataset(datasetDirPath : str) -> Dict:\n",
    "    ''' This functions loads a complete dataset into a dict\n",
    "    \n",
    "    Each Subject contains a dict with 'awake', 'normal', 'fatigue' and 'unlabeled' entry.\n",
    "    Each entry contain the epochSeries and frequencyFeatureDf\n",
    "    '''\n",
    "    \n",
    "    if not os.path.isdir(datasetDirPath):\n",
    "        raise Exception(\"The given dir path '{}' does not exist!\".format(datasetDirPath))\n",
    "    \n",
    "    datasetDict = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(datasetDirPath):\n",
    "        for subjectDir in dirs:\n",
    "            print(\"Load Subject {} Data...\".format(subjectDir))\n",
    "            \n",
    "            epochSeries_awake, frequencyFeatureDf_awake = loadPickeldData(dataDir = os.path.join(datasetDirPath, subjectDir),\n",
    "                                                                          label=\"awake\")\n",
    "            \n",
    "            epochSeries_normal, frequencyFeatureDf_normal = loadPickeldData(dataDir = os.path.join(datasetDirPath, subjectDir),\n",
    "                                                                          label=\"normal\")\n",
    "            \n",
    "            epochSeries_fatigue, frequencyFeatureDf_fatigue = loadPickeldData(dataDir = os.path.join(datasetDirPath, subjectDir),\n",
    "                                                                          label=\"fatigue\")\n",
    "            \n",
    "            epochSeries_unlabeled, frequencyFeatureDf_unlabeled = loadPickeldData(dataDir = os.path.join(datasetDirPath, subjectDir),\n",
    "                                                                          label=\"unlabeled\")\n",
    "            \n",
    "            datasetDict[subjectDir] = {\"awake\" : (epochSeries_awake, frequencyFeatureDf_awake),\n",
    "                                       \"normal\" : (epochSeries_normal, frequencyFeatureDf_normal),\n",
    "                                       \"fatigue\" : (epochSeries_fatigue, frequencyFeatureDf_fatigue),\n",
    "                                       \"unlabeled\" : (epochSeries_unlabeled, frequencyFeatureDf_unlabeled)}\n",
    "    return datasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed the EEG Online Data\n"
     ]
    }
   ],
   "source": [
    "from consts import DEVICES_NEUROSCAN\n",
    "\n",
    "PROCESS_DATA = False\n",
    "\n",
    "if PROCESS_DATA:\n",
    "    # Process the online EEG Data\n",
    "    processRawDatasetToPickleFiles(datasetDirPath = \"D:/Masterthesis/EEG_Data/eeg_data_online\",\n",
    "                              device = DEVICES_NEUROSCAN,\n",
    "                              awakeFileName = None,\n",
    "                              fatigueFileName = \"Fatigue_state_256hz.csv\",\n",
    "                              normalFileName = \"Normal_state_256hz.csv\",\n",
    "                              unlabeledFileName = None)\n",
    "else:\n",
    "    print (\"Already processed the EEG Online Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Subject 1 Data...\n",
      "Load Subject 10 Data...\n",
      "Load Subject 11 Data...\n",
      "Load Subject 12 Data...\n",
      "Load Subject 2 Data...\n",
      "Load Subject 3 Data...\n",
      "Load Subject 4 Data...\n",
      "Load Subject 5 Data...\n",
      "Load Subject 6 Data...\n",
      "Load Subject 7 Data...\n",
      "Load Subject 8 Data...\n",
      "Load Subject 9 Data...\n"
     ]
    }
   ],
   "source": [
    "eegDataset = loadPickeldDataset(datasetDirPath= \"D:/Masterthesis/EEG_Data/eeg_data_online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertFeatureDfToXy(featureDf : pd.DataFrame, target : int) -> (np.ndarray, np.ndarray):\n",
    "    samples = []\n",
    "    targetArray = []\n",
    "    \n",
    "    for index, row in featureDf.iterrows():\n",
    "        timesteps = []\n",
    "        features = row.to_numpy() # features\n",
    "        timesteps.append(features)\n",
    "        samples.append(timesteps)\n",
    "        \n",
    "        targetArray.append(target)\n",
    "    \n",
    "    X = np.array(samples)\n",
    "    y = np.array(targetArray)\n",
    "    \n",
    "    return X, y \n",
    "    \n",
    "\n",
    "def createXyFromDataSeries(dataSeries : pd.Series, target : int) -> (np.ndarray, np.ndarray):\n",
    "    ''' Create X and y for machine learning\n",
    "    \n",
    "    @param pd.Series dataSeries: Should be a series of dataframes\n",
    "    \n",
    "    X should look like this [samples, timesteps, features]\n",
    "        samples: The epoch\n",
    "        timesteps: E.g. if the epoch contains 200 values then the timestep should contain 200 values\n",
    "        features: The actual value\n",
    "    \n",
    "    y should look tlike this [classIds] \n",
    "        classIds: The label for the sample of the X Data\n",
    "    '''\n",
    "    \n",
    "    samples = []\n",
    "    targetArray = []\n",
    "    \n",
    "    if dataSeries is None:\n",
    "        raise TypeError(\"Data Series is None!\")\n",
    "    \n",
    "    if type(dataSeries) != pd.Series:\n",
    "        raise Exception(\"The given dataSeries is not a pd.Series! It is {}\".format(type(dataSeries)))\n",
    "    \n",
    "    # loop through the data Series\n",
    "    for df in dataSeries:\n",
    "        \n",
    "        if type(df) != pd.DataFrame: # check the type\n",
    "            raise Exception(\"The dataseries contains a {} object - The series should dataframes only!\".format(type(df)))\n",
    "            \n",
    "        timesteps = []\n",
    "            \n",
    "        for index, row in df.iterrows():\n",
    "            features = row.to_numpy() # features\n",
    "            timesteps.append(features)\n",
    "        \n",
    "        samples.append(timesteps)\n",
    "        targetArray.append(target)\n",
    "    \n",
    "    \n",
    "    X = np.array(samples)\n",
    "    y = np.array(targetArray)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def createMachineLearningDataset(eegDataset, targetLabelDict) -> (np.array, np.array):\n",
    "    \n",
    "    X = None\n",
    "    y = None\n",
    "    \n",
    "    for subject in eegDataset:\n",
    "        for target in targetLabelDict:\n",
    "            \n",
    "            try:\n",
    "                print(\"Processing Subject {} - Target: {} ...\".format(subject, target))\n",
    "                eegDataset[subject][target][0]\n",
    "                tempX, tempy = createXyFromDataSeries(dataSeries = eegDataset[subject][target][0],\n",
    "                                       target = targetLabelDict[target])\n",
    "\n",
    "                if X is None:\n",
    "                    X = tempX\n",
    "                else:\n",
    "                    X = np.concatenate((X, tempX))\n",
    "\n",
    "                if y is None:\n",
    "                    y = tempy\n",
    "                else:\n",
    "                    y = np.concatenate((y, tempy))\n",
    "            \n",
    "            except TypeError:\n",
    "                print(\"Skipping Target: {}\".format(target))\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return X,y\n",
    "\n",
    "def saveDictToFile(myDict, filepath : str):\n",
    "    ''' Save a dictionary the given file path'''\n",
    "    \n",
    "    print(\"Saving dict to {}\".format(filepath))\n",
    "    f = open(filepath, \"w\")\n",
    "    for key, value in myDict.items():\n",
    "        line = \"{v} {k}\\n\".format(v=value, k=key.upper())\n",
    "        f.write(str(line))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\n",
    "def createAndSafeMlDataset(eegDataset : Dict[str, Dict[str ,Tuple[pd.Series, pd.DataFrame]]], targetLabelDict : Dict,\n",
    "                           dirPath : str) -> (np.array, np.array):\n",
    "    \n",
    "    if not os.path.isdir(dirPath):\n",
    "        raise Exception(\"The given directory path is not valid! Given path: {}\".format(dirPath))\n",
    "    \n",
    "    print(\"Creating Machine Learning Dataset!\")\n",
    "    X, y = createMachineLearningDataset(eegDataset, targetLabelDict)\n",
    "    \n",
    "    #Todo - Build the feature df somehow into that too\n",
    "    \n",
    "    print(\"\\nSaving Machine Learning Dataset into this directory: {}\".format(dirPath))\n",
    "    np.save(os.path.join(dirPath, \"data_series_X.npy\"), X)\n",
    "    np.save(os.path.join(dirPath, \"data_series_y.npy\"), y)\n",
    "    \n",
    "    # Save target labels\n",
    "    saveDictToFile(targetLabelDict, filepath=os.path.join(dirPath,'target_labels.txt'))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "#createDatasetXy(dataSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already created ML Data\n"
     ]
    }
   ],
   "source": [
    "targetLabelDict = {\"normal\" : 0,\n",
    "                   \"fatigue\" : 1,\n",
    "              \"awake\" : 2,\n",
    "              \"unlabeled\" : 3}\n",
    "\n",
    "CREATE_ML_DATA = False\n",
    "\n",
    "#X, y = createMachineLearningDataset(eegDataset, targetLabelDict)\n",
    "# # Todo epoch series UND frequency df X und y erstellen!!\n",
    "\n",
    "if CREATE_ML_DATA:\n",
    "    createAndSafeMlDataset(eegDataset=eegDataset,\n",
    "                           targetLabelDict=targetLabelDict,\n",
    "                           dirPath=\"D:/Masterthesis/EEG_Data/eeg_data_online\")\n",
    "else:\n",
    "    print(\"Already created ML Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadOnlineEEGdata(test_size=0.3, shuffle=False):\n",
    "    dirPath = \"D:/Masterthesis/EEG_Data/eeg_data_online\"\n",
    "    \n",
    "    # load array\n",
    "    X = np.load(os.path.join(dirPath, 'data_series_X.npy'))\n",
    "    y = np.load(os.path.join(dirPath, 'data_series_y.npy'))\n",
    "    \n",
    "    # load target labels\n",
    "    \n",
    "    # load feature names\n",
    "    \n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,random_state=109, shuffle=shuffle) # 70% training and 30% test\n",
    "    \n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5024, 512, 40) (5024, 2) (2154, 512, 40) (2154, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = loadOnlineEEGdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 15, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    print(\"n_timesteps: {} - n_features: {} - n_outputs: {}\".format(n_timesteps, n_features, n_outputs))\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats=10):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = loadOnlineEEGdata()\n",
    "    print(\"Loaded the Dataset\")\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5024, 512, 40) (5024, 2) (2154, 512, 40) (2154, 2)\n",
      "Loaded the Dataset\n",
      "n_timesteps: 512 - n_features: 40 - n_outputs: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-May-20 15:41:29 | WARNING | From C:\\Users\\reiss\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_experiment(repeats=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
